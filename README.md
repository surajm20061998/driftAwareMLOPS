ğŸ§  Drift-Aware MLOps System (Prod + Canary)

This repository implements a drift-aware machine learning deployment system with:
	â€¢	Offline drift detection (PSI)
	â€¢	Automated retraining
	â€¢	MLflow Model Registry
	â€¢	Production + Canary inference services
	â€¢	Postgres inference logging
	â€¢	MinIO (S3) artifact storage
	â€¢	Manual promotion workflow
	â€¢	Observability hooks (Prometheus-ready)

The system continuously monitors feature drift and enables safe, controlled rollout of new models via canary deployments.

â¸»

ğŸ—ï¸ Architecture Overview

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  MinIO     â”‚
                        â”‚  (S3)      â”‚
                        â”‚ artifacts  â”‚
                        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Drift Job     â”‚â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–¶â”‚ MLflow Server  â”‚
        â”‚ (PSI)         â”‚     â”‚      â”‚ Model Registry â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚               â”‚              â”‚
              â”‚ drift=true    â”‚              â”‚
              â–¼               â”‚              â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Retraining      â”‚â”€â”€â”€â”€â”€â”˜     â”‚ Inference Prod   â”‚
     â”‚ (local / CI)    â”‚           â”‚ MODEL_STAGE=Prod â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                               â”‚
           â”‚ new version (Staging)         â”‚
           â–¼                               â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Inference Canary â”‚           â”‚ Postgres          â”‚
 â”‚ MODEL_STAGE=Stg â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ inference_logs    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â¸»

ğŸš€ Components

1ï¸âƒ£ Drift Detection (drift/drift_job.py)
	â€¢	Pulls recent inference data from Postgres
	â€¢	Computes Population Stability Index (PSI)
	â€¢	Compares against baseline distribution
	â€¢	Uploads drift report to MinIO
	â€¢	Exits with:
	â€¢	0 â†’ no drift
	â€¢	2 â†’ drift detected (used by shell script)

â¸»

2ï¸âƒ£ Automated Retraining (drift/retrain_if_drifted.sh)
	â€¢	Runs drift job
	â€¢	If drift detected:
	â€¢	Triggers retraining
	â€¢	Logs model to MLflow
	â€¢	Registers new version in Staging
	â€¢	Designed to run locally or in CI

â¸»

3ï¸âƒ£ Model Registry (MLflow)
	â€¢	Centralized model tracking
	â€¢	Uses stages (Production / Staging / Archived)
	â€¢	Models stored in MinIO (s3://mlflow)
	â€¢	Promotion handled via MLflow API

â¸»

4ï¸âƒ£ Inference Services (FastAPI)

Service	Port	Model Stage	Deployment
inference	8000	Production	legacy
inference_prod	8001	Production	prod
inference_canary	8002	Staging	canary

Each service:
	â€¢	Loads model from MLflow
	â€¢	Serves /predict
	â€¢	Logs inference metadata to Postgres
	â€¢	Exposes /metrics for Prometheus

â¸»

5ï¸âƒ£ Inference Logging (Postgres)

Table: inference_logs

CREATE TABLE inference_logs (
  id BIGSERIAL PRIMARY KEY,
  ts TIMESTAMPTZ DEFAULT now(),
  model_name TEXT NOT NULL,
  model_version TEXT NOT NULL,
  latency_ms DOUBLE PRECISION NOT NULL,
  features JSONB NOT NULL,
  prediction DOUBLE PRECISION NOT NULL,
  deployment TEXT NOT NULL
);

This enables:
	â€¢	Prod vs canary comparison
	â€¢	Latency analysis
	â€¢	Drift analysis per deployment
	â€¢	Rollback decisions

â¸»

ğŸ§ª Example Usage

Health Checks

curl http://localhost:8001/health   # prod
curl http://localhost:8002/health   # canary

Prediction

curl -X POST http://localhost:8001/predict \
  -H "Content-Type: application/json" \
  -d '{"features":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]}'


â¸»

ğŸ“Š Compare Prod vs Canary

SELECT deployment,
       COUNT(*) AS n,
       AVG(latency_ms) AS avg_latency,
       AVG(prediction) AS avg_pred
FROM inference_logs
GROUP BY deployment;


â¸»

ğŸš¦ Canary Promotion (Manual)

Once canary behavior is acceptable:

curl -X POST http://localhost:5050/api/2.0/mlflow/model-versions/transition-stage \
  -H "Content-Type: application/json" \
  -d '{
    "name": "DriftAwareDemoModel",
    "version": "3",
    "stage": "Production",
    "archive_existing_versions": true
  }'

Then restart prod:

docker restart mlops_inference_prod


â¸»

ğŸ” Observability
	â€¢	Prometheus metrics exposed at /metrics
	â€¢	Latency histogram
	â€¢	Request counters
	â€¢	Model load success/failure

â¸»

ğŸ§± Stack
	â€¢	ML Framework: scikit-learn
	â€¢	Serving: FastAPI + Uvicorn
	â€¢	Registry: MLflow
	â€¢	Storage: MinIO (S3)
	â€¢	DB: Postgres 16
	â€¢	Orchestration: Docker Compose
	â€¢	Monitoring: Prometheus-ready

â¸»

